{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read IEX_TOKEN and IEX_API_VERSION from .env file\n",
    "\n",
    "import os\n",
    "import iexfinance\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Setup cache for requests\n",
    "from datetime import datetime\n",
    "from iexfinance.stocks import Stock\n",
    "import requests_cache\n",
    "import datetime\n",
    "\n",
    "\n",
    "with open('.env', 'r') as env:\n",
    "    os.environ.update({line.split('=')[0].strip(): line.split('=')[1].strip() for line in  filter(lambda li: '=' in li, env.read().split('\\n'))})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade1 = {\n",
    "    'no_shares': 168.0000,\n",
    "    'buy_price': 29.63,\n",
    "    'ticker': 'VUKE.L',\n",
    "    'date': datetime(month=3, day=2, year=2021)\n",
    "    \n",
    "}\n",
    "\n",
    "trade2 = {\n",
    "    'no_shares': 58,\n",
    "    'buy_price': 34.01,\n",
    "    'ticker': 'VMID.L',\n",
    "    'date': datetime(month=3, day=2, year=2021)\n",
    "}\n",
    "\n",
    "\n",
    "def print_trade_returns(trade):\n",
    "    stock = Stock(trade.get('ticker'))\n",
    "    price = stock.get_price()\n",
    "    price = price[trade.get('ticker')].values[0]\n",
    "    value = price * trade.get('no_shares')\n",
    "    returns = (price - trade.get('buy_price'))/price\n",
    "    print(f'value: {value}, return: {returns*100}%')\n",
    "    return value\n",
    "\n",
    "\n",
    "print_trade_returns(trade1) + print_trade_returns(trade2) + 112.27\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expiry = datetime.timedelta(days=100)\n",
    "session = requests_cache.CachedSession(cache_name='cache',\n",
    "                                       backend='sqlite',\n",
    "                                       expire_after=expiry)\n",
    "\n",
    "vuke = Stock('VUKE-LN')\n",
    "\n",
    "\"\"\"\n",
    "purchased on \n",
    "Order summary\n",
    "Estimated value more information £5,000.00 are\n",
    "Order number 5674775\n",
    "Status updated\n",
    "Date 03 March 2021\n",
    "Time 10:16\n",
    "Actual contract cost\n",
    "Value £4,977.62\n",
    "Settlement price £29.63\n",
    "Contract total £4,977.62\n",
    "\"\"\"\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Static data\n",
    "emea_cty_codes = ['AT','BE','BG','HR','CY','CZ','DK','EE','FI','FR','DE','GR','HU','IE','IT','LV','LT','LU','MT','NL','PL','PT','RO','SK','SI','ES','SE','GB']\n",
    "\n",
    "with open('./data/iex_exchanges.json', 'r') as fil:\n",
    "    df = pd.read_json(fil)\n",
    "iex_exchanges = df\n",
    "\n",
    "from iexfinance.refdata import get_region_symbols\n",
    "\n",
    "def download_symbol_static():\n",
    "    df = pd.DataFrame()\n",
    "    for cty_code in emea_cty_codes:\n",
    "        if cty_code in iex_exchanges.region.values:\n",
    "            symbols = get_region_symbols(cty_code)\n",
    "            if len(symbols) == 0:\n",
    "                print(f'cty_code={cty_code} not found')\n",
    "                continue\n",
    "            df = df.append(symbols)\n",
    "        else:\n",
    "            print(f'cty_code={cty_code} is not supported by iex')\n",
    "    all_emea_symbols = df\n",
    "    all_emea_symbols.to_csv('data/all_emea_symbols.csv')\n",
    "    return all_emea_symbols\n",
    "\n",
    "# EMEA Universe\n",
    "all_emea_symbols = pd.read_csv('./data/all_emea_symbols.csv')\n",
    "\n",
    "# FTSE 100 data\n",
    "ftse_companies = pd.read_csv('./data/ftse100.csv')\n",
    "ftse_tickers = list(map(lambda tick: f'{tick}-LN', ftse_companies.Code))\n",
    "\n",
    "# data on vanguard etfs\n",
    "vg_etfs = pd.read_csv('/home/rory/dev/investment-analysis/data/vanguard_fund_summaries.csv')\n",
    "vanguard_funds = all_emea_symbols[all_emea_symbols.name.str.contains('Vanguard')]\n",
    "vanguard_funds.name\n",
    "\n",
    "# Vanguard etf tickers\n",
    "tickers=list(map(lambda it: it.replace(' ', '-'), list(vg_etfs.Bloomberg)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Model Features\n",
    "################\n",
    "# VG ETF prices\n",
    "vg_etf_basket = Stock(tickers, session=session)\n",
    "vg_etf_prices = vg_etf_basket.get_historical_prices(range='5y')\n",
    "vg_etf_prices = process_basket_data(vg_etf_prices)\n",
    "\n",
    "# VG Momentum funds\n",
    "momentum_etfs = Stock(list(vanguard_funds[vanguard_funds.symbol.str.contains('VMOM')].symbol.values), session=session)\n",
    "momentum_prices = momentum_etfs.get_historical_prices(range='5y')\n",
    "momentum_prices = process_basket_data(momentum_prices)\n",
    "\n",
    "# FTSE 100 names\n",
    "ftse_basket = Stock(list(ftse_tickers[0:100]), session=session)\n",
    "ftse_basket_prices = ftse_basket.get_historical_prices(range='5y')\n",
    "ftse_basket_prices = process_basket_data(ftse_basket_prices)\n",
    "\n",
    "# Filter out columns with too few observations\n",
    "mask = [tick for tick in filter(lambda ticker: ftse_basket_prices.loc[ticker].close.size >=1200, ftse_basket_prices.index.levels[0])]\n",
    "ftse_basket_prices = ftse_basket_prices.loc[mask]\n",
    "\n",
    "\n",
    "####################\n",
    "# Prediction Targets\n",
    "####################\n",
    "# FTSE100 INDEX\n",
    "ftse100 = Stock('VUKE-LN', session=session)\n",
    "ftse100_prices = ftse100.get_historical_prices(range='5y')\n",
    "ftse100_prices.index = pd.to_datetime(ftse100_prices.index)\n",
    "\n",
    "ftse250 = Stock('VMID-LN', session=session)\n",
    "ftse250_prices = ftse250.get_historical_prices(range='5y')\n",
    "ftse250_prices.index = pd.to_datetime(ftse250_prices.index)\n",
    "ftse250_prices\n",
    "\n",
    "\n",
    "ftse_basket_prices.index.levels[0]\n",
    "\n",
    "ftse_prediction = normalise_basket(ftse_basket_prices, 'returns').dropna()\n",
    "ftse_prediction['index'] = daily_returns(ftse100_prices.close.astype('float'))\n",
    "ftse_prediction['target'] = ftse_prediction['index'].astype('float').apply(lambda ret: 1 if ret > 0 else 0)\n",
    "\n",
    "ftse_prediction = ftse_prediction.dropna()[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftse_prediction['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "con=sqlite3.connect('./example.db')\n",
    "ftse_prediction.to_sql(name='ftse_prediction', if_exists='replace', con=con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply lag to x of 1, so we only ever use yesterday's close price of components\n",
    "period_lag = -1\n",
    "ftse_prediction['target'] = ftse_prediction.target.shift(period_lag)\n",
    "ftse_prediction['index'] = ftse_prediction['index'].shift(period_lag)\n",
    "#ftse_prediction = ftse_prediction[period_lag:]\n",
    "\n",
    "\n",
    "#################\n",
    "# Features Subset\n",
    "#################\n",
    "test_cols = list(filter(lambda it: it not in [ 'index', 'target'] and 'minus' not in it, ftse_prediction.columns))\n",
    "for col in test_cols:\n",
    "    for i in range(1,6):\n",
    "        ftse_prediction[f'{col}_tminus{i}'] = ftse_prediction[col].shift(i)\n",
    "test_cols = list(filter(lambda it: it not in [ 'index', 'target'], ftse_prediction.columns))\n",
    "\n",
    "ftse_prediction.dropna(inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "con = sqlite3.connect('example.db')\n",
    "\n",
    "\n",
    "\n",
    "ftse_prediction = pd.read_sql('select * from ftse_prediction', con)\n",
    "test_cols = list(filter(lambda it: it not in [ 'index', 'target'], ftse_prediction.columns))\n",
    "ftse_prediction.index = pd.to_datetime(ftse_prediction['level_0'])\n",
    "ftse_prediction = ftse_prediction[['target', 'index'] + test_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftse_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = ftse_prediction[list(filter(lambda ticker: ticker not in  ['index', 'target'], ftse_prediction.columns))]\n",
    "\n",
    "#basket_returns = ftse_prediction#normalise_basket(df, 'returns')\n",
    "pca = PCA()\n",
    "pca.fit(df)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "y = np.round(pca.explained_variance_ratio_* 100, decimals =2)\n",
    "plt.bar(x=range(0, pca.components_[0].size), height=y)\n",
    "plt.xticks(range(0, pca.components_[0].size), rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Visualise first component\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(x=range(0, pca.components_[0].size), height=pca.components_[0])\n",
    "plt.xticks(range(0, pca.components_[0].size), df.columns, rotation=90)\n",
    "plt.show()\n",
    "\n",
    "ftse_pca_score = pca.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Train/Test subsetting\n",
    "#######################\n",
    "from datetime import datetime\n",
    "\n",
    "train_start = pd.to_datetime('16/05/2019')\n",
    "train_end = pd.to_datetime('31/01/2021')\n",
    "\n",
    "train_mask = lambda s: (s.index > train_start) & (s.index <= train_end)\n",
    "test_mask = lambda s: (s.index > train_end) & (s.index <= datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Lasso regression\n",
    "#  y: ftse price \n",
    "#  X: basket of stocks\n",
    "######################\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "X_train=ftse_prediction[train_mask(ftse_prediction)][test_cols]\n",
    "y_train=ftse_prediction[train_mask(ftse_prediction)]['index']\n",
    "\n",
    "X_test = ftse_prediction[test_mask(ftse_prediction)][test_cols]\n",
    "y_test = ftse_prediction[test_mask(ftse_prediction)]['index']\n",
    "clf = Lasso(alpha=0.1)\n",
    "clf.fit(X=X_train, y=y_train)\n",
    "print(f'Train Score: {clf.score(X=X_train, y=y_train)}')\n",
    "print(f'Model score: {clf.score(X=X_test, y=y_test)}')\n",
    "clf.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit an ARIMA model and plot residual errors\n",
    "from pandas import datetime\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from matplotlib import pyplot\n",
    "# load dataset\n",
    "\n",
    "series = ftse_prediction['index'].astype('float')\n",
    "\n",
    "# fit model\n",
    "model = ARIMA(series, order=(5,1,0))\n",
    "model_fit = model.fit()\n",
    "# summary of fit model\n",
    "print(model_fit.summary())\n",
    "# line plot of residuals\n",
    "residuals = DataFrame(model_fit.resid)\n",
    "residuals.plot()\n",
    "pyplot.show()\n",
    "# density plot of residuals\n",
    "residuals.plot(kind='kde')\n",
    "pyplot.show()\n",
    "# summary stats of residuals\n",
    "print(residuals.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# Lasso regression on classification target\n",
    "# of increase or decrease daily returns\n",
    "###########################################\n",
    "\n",
    "X_train=ftse_prediction[train_mask(ftse_prediction)][list(filter(lambda it: it not in ['index','target'], ftse_prediction.columns))]\n",
    "y_train=ftse_prediction[train_mask(ftse_prediction)]['target'].astype('int')\n",
    "\n",
    "X_test = ftse_prediction[test_mask(ftse_prediction)][list(filter(lambda it: it not in ['index','target'], ftse_prediction.columns))]\n",
    "y_test = ftse_prediction[test_mask(ftse_prediction)]['target'].astype('int')\n",
    "clf = Lasso(alpha=0.001)\n",
    "clf.fit(X=X_train, y=y_train)\n",
    "clf.score(X=X_test, y=y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import  NearestCentroid\n",
    "train_mask(ftse_prediction)\n",
    "\n",
    "features_train = ftse_prediction[train_mask(ftse_prediction)][test_cols]\n",
    "features_test = ftse_prediction[test_mask(ftse_prediction)][test_cols]\n",
    "target_train = ftse_prediction[train_mask(ftse_prediction)][['target']].astype('int')\n",
    "target_test = ftse_prediction[test_mask(ftse_prediction)]['target'].astype('int')\n",
    "\n",
    "pca = PCA()\n",
    "target = ftse_prediction['target']\n",
    "\n",
    "clf = NearestCentroid()\n",
    "\n",
    "#pca.fit(features_train)\n",
    "#features_train = pca.transform(features_train)\n",
    "#features_test = pca.transform(features_test)\n",
    "clf.fit(X=features_train, y=target_train)\n",
    "\n",
    "clf.score(y=target_test, X=features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "lr_list = [0.75, 1, 1.1, 1.2, 1.3, 1.4]\n",
    "\n",
    "for learning_rate in lr_list:\n",
    "    gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=learning_rate, max_features=30, max_depth=10, random_state=0)\n",
    "    gb_clf.fit(X_train, y_train)\n",
    "    print(\"Learning rate: \", learning_rate)\n",
    "    print(\"Accuracy score (training): {0:.3f}\".format(gb_clf.score(X_train, y_train)))\n",
    "    print(\"Accuracy score (validation): {0:.3f}\".format(gb_clf.score(X_test, y_test)))\n",
    "    \n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.75, max_features=30, max_depth=10, random_state=0)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "print(\"Accuracy score (training): {0:.3f}\".format(gb_clf.score(X_train, y_train)))\n",
    "print(\"Accuracy score (validation): {0:.3f}\".format(gb_clf.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = gb_clf\n",
    "\n",
    "clf.predict(features_test)\n",
    "pca.fit(features_train)\n",
    "feat_test_df = pd.DataFrame(pca.transform(features_test))\n",
    "prediction = clf.predict(features_test)\n",
    "\n",
    "\n",
    "def printtable():\n",
    "    print(f'|Actual| Pred| PC1| PC2')\n",
    "    print(f'--------------')\n",
    "\n",
    "    for pred, act, pc1, pc2, targ in zip(clf.predict(features_test), \n",
    "                                         target_test, \n",
    "                                         feat_test_df[0], \n",
    "                                         feat_test_df[1], \n",
    "                                         ftse_prediction['index']):\n",
    "        print('     {}|    {}|   {:0.2f}|   {:0.2f}|  {:0.2f}'.format(act, pred, pc1, pc2, targ))\n",
    "        print('--------------')\n",
    "\n",
    "\n",
    "cols = ['Actual', 'Predicted', 'PC1', 'PC2']\n",
    "vals = [target_test, clf.predict(features_test)]\n",
    "\n",
    "to_plot = pd.DataFrame()\n",
    "\n",
    "to_plot['PC1'] = feat_test_df[0]\n",
    "to_plot['PC2'] = feat_test_df[1]\n",
    "to_plot['PC3'] = feat_test_df[2]\n",
    "to_plot['PC4'] = feat_test_df[3]\n",
    "to_plot['Actual'] = target_test.values\n",
    "to_plot['Predicted'] = clf.predict(features_test)\n",
    "fig, ax = plt.subplots()\n",
    "colors = {0: 'red', 1: 'green'}\n",
    "ax.scatter(to_plot['PC2'], to_plot['PC3'], c=to_plot['Actual'].map(colors))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = to_plot.plot.scatter(x='PC1', y='PC2', color=to_plot['Predicted'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(filter( lambda model: model.ready(), models))\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params={'n_estimators': 10000, 'learning_rate': 0.01, 'max_features': 30, 'max_depth': 4, 'random_state': 0}\n",
    "\n",
    "parameter_space = {\n",
    "    \"alpha\": [0.1, 0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "    'learning_rate': [0.1,0.2,0.3,0.4,0.5],\n",
    "    'max_depth': [2,4,6,8,10],\n",
    "    'max_features': [10,20,30,40,50,60],\n",
    "    'min_samples_leaf': [1,2,3],\n",
    "    'min_samples_split': [2,5,10],\n",
    "    'n_estimators': [500,1000,2000,3000,4000,5000],\n",
    "    'random_state': [0,1,2,3,4,5,6],\n",
    "    'warm_start': [False,True]\n",
    "}\n",
    "\n",
    "#base_estimator = GradientBoostingRegressor()\n",
    "#gb_reg.fit(X_train, y_train)\n",
    "\n",
    "sh = GridSearchCV(GradientBoostingRegressor(), parameter_space, n_jobs=8, pre_dispatch=16)\n",
    "sh.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_reg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train=ftse_prediction[train_mask(ftse_prediction)][test_cols]\n",
    "y_train=ftse_prediction[train_mask(ftse_prediction)]['target'].astype('int')\n",
    "X_test = ftse_prediction[test_mask(ftse_prediction)][test_cols]\n",
    "y_test = ftse_prediction[test_mask(ftse_prediction)]['target'].astype('int')\n",
    "\n",
    "clf = SVC(kernel='sigmoid')\n",
    "\n",
    "clf.fit(X=features_train,y=target_train)\n",
    "y_pred = clf.predict(features_test)\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SVC.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_week = ftse_basket.get_historical_prices(range='1w')\n",
    "\n",
    "last_week = process_basket_data(last_week)\n",
    "last_week_data = normalise_basket(last_week, 'returns')\n",
    "last_week_data\n",
    "\n",
    "features = last_week_data[list(filter(lambda col: col not in ['filter', 'target'], last_week_data.columns))]\n",
    "\n",
    "features = features[features.replace([np.inf, -np.inf], np.nan).notnull()]\n",
    "features = features[filter(lambda item: item not in ['target', 'index'] ,ftse_prediction.columns)]\n",
    "\n",
    "last_week_target = ftse100.get_historical_prices(range='1w').close.astype('float').pct_change()\n",
    "features['target'] = last_week_target\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gb_clf.predict(features[test_cols].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_week_target"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
